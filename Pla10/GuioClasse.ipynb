{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecció d'objectes\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\">\n",
    "            <img src=\"Imatges/class-local-detec-segm.jpeg\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\" align=center >\n",
    "            Font imatge: <a href=\"https://medium.com/zylapp/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852\">Medium </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentació d'imatges \n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\">\n",
    "            <img src=\"Imatges/segmentacio-semantica.png\">\n",
    "        </td>\n",
    "        <td style=\"border:1px solid black\">\n",
    "            <img src=\"Imatges/segmentacio-instancia.jpg\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\">\n",
    "            Segmentació semàntica: cada píxel és etiquetat.\n",
    "        </td>\n",
    "        <td style=\"border:1px solid black\">\n",
    "            Segmentació d'instàncies: s'etiqueten els píxels d'una mateixa instància detectada.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\">\n",
    "            Font imatge: <a href=\"http://sqlml.azurewebsites.net/2017/09/12/convolutional-neural-network/\">SQLML</a>\n",
    "        </td>\n",
    "        <td style=\"border:1px solid black\">\n",
    "            Font imatge: <a href=\"https://www.pyimagesearch.com/2018/11/26/instance-segmentation-with-opencv/\">PyImageSearch</a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sliding window\n",
    "\n",
    "Com ho faríeu?\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td  style=\"border:1px solid black\" width=350>\n",
    "            <img src=\"Imatges/sliding-window-animated-sot.gif\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\" >\n",
    "            Exemple de finestra lliscant i escalatge de la imatge original per a la detecció d'objectes.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\" align=center >\n",
    "            Font imatge: <a href=\"https://www.pyimagesearch.com/2015/03/23/sliding-windows-for-object-detection-with-python-and-opencv/\">PyImageSearch </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"Imatges/sliding_window.jpg\">\n",
    "\n",
    "Idea: recòrrer la imatge amb diferents mides de finestra, reescalar els patches i passar un classificador\n",
    "\n",
    "Pseudo-codi:\n",
    "\n",
    "```\n",
    "for window in windows\n",
    "    patch = get_patch(image, window)\n",
    "    resized_patch = adapt_size(patch)\n",
    "    results = detector(resized_patch)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R-CNN\n",
    " \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td  style=\"border:1px solid black\" width=350>\n",
    "            <img src=\"Imatges/rcnn.png\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\" >\n",
    "            R-CNN\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\" align=center >\n",
    "            Font imatge: <a href=\"https://arxiv.org/pdf/1311.2524.pdf\">Ross Girshick et al. \"*Rich feature hierarchies for accurate object detection and semantic segmentation*\" </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "IDEA: Fent servir selective seach, reduir el nombre de ROIS (unes 2000)\n",
    "\n",
    "Pseudo-codi:\n",
    "\n",
    "```\n",
    "ROIs = region_proposal(image)\n",
    "for ROI in ROIs\n",
    "    patch = get_patch(image, ROI)\n",
    "    results = detector(patch)\n",
    "```\n",
    "\n",
    "<img src='Imatges/selective_seach.png'>\n",
    "\n",
    "* Propietats: \n",
    "\n",
    " - Més eficient que sliding window\n",
    " - Selective search i classificador s'entrenen per separat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast-RCNN\n",
    "\n",
    "IDEA: Reutilitzar l'extracció de característiques per a totes les rois\n",
    "\n",
    "Pseudo-codi:\n",
    "\n",
    "```\n",
    "feature_maps = process(image)\n",
    "ROIs = region_proposal(image)\n",
    "for ROI in ROIs\n",
    "    patch = roi_pooling(feature_maps, ROI)\n",
    "    results = detector2(patch)\n",
    "```\n",
    "\n",
    "* Propietats: \n",
    " - Guany de 10X entrenant i de 150X inferint respecte R-CNN\n",
    " - End2End: S'entrena de forma simultànea el classificador i el localitzador\n",
    " - Roi pooling: Adapta les diferents mides de finestres a una mida fixa\n",
    " - Region proposal és idèntic a RCNN (treballa sobre imatges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster RCNN\n",
    "\n",
    "IDEA: Optimitzar el region proposal (en Fast-RCNN és el que consumeix més temps)\n",
    "\n",
    "Pseudo-codi:\n",
    "\n",
    "```\n",
    "feature_maps = process(image)\n",
    "ROIs = region_proposal(feature_maps[0])\n",
    "for ROI in ROIs\n",
    "    patch = roi_pooling(feature_maps, ROI)\n",
    "    results = detector2(patch)\n",
    "```\n",
    "\n",
    "* Propietats:\n",
    " - Té una xarxa neuronal per al region proposal.\n",
    " - Per tal de facilitar les coses, algoritme pot escollir entre 9 possibles regions per cada punt (3 escales i 3 orientacions)\n",
    " - Per cada regió, aplica un classificador que li diu si és objecte o background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparativa 2 phases detectors\n",
    "\n",
    "<img src='Imatges/comparativa1.jpeg'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single shot detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEA: Fusionar la proposició de finestres amb la detecció\n",
    "\n",
    "IDEA2: No cal predir directament les bounding boxes sobre la imatge, podem treballar directament sobre el feature map\n",
    "    \n",
    "<img src='Imatges/64locations.png'>\n",
    "\n",
    "Per cada punt, podem predir una sèrie de bounding boxes \n",
    "\n",
    "<img src='Imatges/anchors.png'>\n",
    "\n",
    "Aquí es mostra com per cada anchor, es fan una sèrie de prediccions\n",
    "\n",
    "<img src='Imatges/predictions_anchors.jpeg'>\n",
    "\n",
    "A diferència de Faster-RCNN, Single shot detectors prediuen N classes a la vegada\n",
    "\n",
    "\n",
    "\n",
    "* Propietats\n",
    " - Més ràpids que Faster-RCNN\n",
    " - Tenen problemes per detectar objectes molt petits i propers entre ells\n",
    " - Exemples: SSD i YoloV3\n",
    " \n",
    " \n",
    "<img src='Imatges/comparativa2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiescala:\n",
    "\n",
    "<img src='Imatges/ssd.jpeg'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class imbalance\n",
    "\n",
    "El nombre de finestres sense objectes a detectar (background) és molt més gran que el foreground\n",
    "\n",
    "Alternatives:\n",
    "- SSD limita amb un rati 1:3 els positius vs els negatius (triant els que més loss aporten)\n",
    "- RetinaNET: Canvia la funció de loss respecte CrossEntropy, penalitzant les classes més freqüents\n",
    "\n",
    "\n",
    "<img src='Imatges/focalloss.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non maximal supression\n",
    "\n",
    "<img src='Imatges/nms.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparativa\n",
    "\n",
    "Paràmetres a escollir:\n",
    " - Backbone (feature extractor)\n",
    " - Data augmentation per millorar generalització i reduir overfitting\n",
    " - Single shot (fast) vs Dual Shot (precís)\n",
    "\n",
    "<img src='Imatges/comparativa3.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exercici resolt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
