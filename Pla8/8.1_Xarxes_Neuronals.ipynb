{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 Xarxes Neuronals \n",
    "\n",
    "## Introducció a les Xarxes Neuronals\n",
    "Les xarxes neuronals són sistemes d’intel·ligència artificial inspirats en les xarxes neuronals biològiques. \n",
    "\n",
    "La neurona és la unitat fonamental del sistema nerviós i del cervell. Cada neurona és una unitat simple processadora que rep i combina senyals \"*des de*\" i \"*cap a*\" altres neurones. \n",
    "\n",
    "El nostre cervell funciona gràcies a una **xarxa de neurones interconnectades**, cadascuna de les quals és excitada o activada per un tipus d’input concret que recullen les dendrites (escalfor, llum, tacte, etc.), aquesta informació es processa i es transmet al canal de sortida que és l’axó. Aquest axó, a la seva vegada, excita les dendrites de la neurona següent i es torna a repetir el procés fins arribar al cervell on es dedueixen les conclusions i sortides o accions finals a realitzar (apartar la mà si ens estem cremant, tancar els ulls si ens està enlluernant una llum molt forta, etc.)\n",
    "\n",
    "Igual que l'entrada de la neurona té ramificacions (les dendrites), la sortida (l'axó) també en té. Les ramificacions de sortida d'una neurona estan connectades amb les entrades ramificades de la següent neurona a través de les unions conegudes com sinapsis. \n",
    "\n",
    "Les xarxes neuronals artificials emulen aquesta funcionalitat biològica a través del que s’anomena **neurona artificial** o PE (de l'anglès *Process Element*), les quals, interconnectades entre elles, creen el que es coneix com **xarxes neuronals artificials** (ANN en anglès o RNA en castellà).\n",
    "\n",
    " \n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\" width=350>\n",
    "            <img src=\"Imatges/neurona.png\">\n",
    "        </td>\n",
    "        <td style=\"border:1px solid black\" width=350>\n",
    "            <img src=\"Imatges/neurona_artificial.png\">            \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\" align=center>\n",
    "            Neurona.<br>\n",
    "            Font: <a href=\"https://en.wikipedia.org/wiki/Artificial_neural_network\">Wikipedia</a>\n",
    "        </td>\n",
    "        <td style=\"border:1px solid black\" align=center>\n",
    "            Neurona artificial.<br>\n",
    "            Font: <a href=\"https://es.wikipedia.org/wiki/Perceptr%C3%B3n\">Wikipedia</a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "    \n",
    "\n",
    "La neurona artificial bàsica té diversos receptors d’entrada de dades en forma de vector ($\\begin{equation}x_1, x_2, ..., x_n\\end{equation}$), els quals s’activen només sota unes característiques concretes, i combina les diferents entrades en un sol valor de sortida $\\begin{equation}y\\end{equation}$ gràcies a la funció sumadora $\\begin{equation}f(x)\\end{equation}$. \n",
    "\n",
    "La funció sumadora fa servir un vector de pesos $\\begin{equation}w_1, w_2, ..., w_i\\end{equation}$ per calcular la sortida. Els pesos fan augmentar o disminuir la força del senyal d'entrada sobre la sortida final. \n",
    "\n",
    "A més, a l’igual que les neurones reals tenen un grau d’inhibició, les neurones artificials també poden tenir un llindar per activar la funció d’activació.\n",
    "\n",
    "Les neurones artificials s'organitzen en **capes** (grups d'un mateix nivell), i cada neurona està connectada amb una o diverses neurones de la capa superior, creant així les xarxes neuronals artificials. \n",
    "\n",
    "L’arquitectura bàsica d’una xarxa neuronal artificial està formada per 3 capes: la d’entrada, l’oculta de processament i la de sortida. Segons el tipus d'arquitectura utilitzada, la capa oculta pot tenir cap o bé diverses capes de neurones. \n",
    " \n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\" align=center>\n",
    "            <img src=\"Imatges/xarxa_neuronal.png\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\" align=center>\n",
    "            Xarxa Neuronal: cada cercle és una neurona, <br>\n",
    "            cada fletxa és una connexió entre neurones.<br>\n",
    "            Font: <a href=\"https://es.wikipedia.org/wiki/Red_neuronal_artificial\">Wikipedia</a>\n",
    "        </td>\n",
    "    </tr>   \n",
    "</table>\n",
    "\n",
    "\n",
    "A més de la semblança arquitectònica amb les xarxes neuronals biològiques,  cal destacar que una xarxa neuronal artificial presenta 3 propietats que també són pròpies de les xarxes biològiques:\n",
    "1. Aprenen de l’experiència: la xarxa pot canviar el seu comportament segons l’entorn; una mateixa xarxa es pot comportar de manera diferent segons les dades d’entrada.\n",
    "2. Generalitzen d’exemples previs a exemples nous, fins i tot davant limitats efectes de soroll i distorsió de les dades d’entrada.\n",
    "3. Abstreuen les característiques principals d’una sèrie de dades. Algunes xarxes són capaces d’extreure l’essència d’un conjunt d’entrades que a primera vista no presenten aspectes comuns.\n",
    "\n",
    "Aquests sistemes aprenen *quasi* sols. Enlloc de programar de forma explícita cada característica a tenir en compte, cada norma important per deduir una classificació determinada, les xarxes neuronals s'auto-regulen per aprendre i detectar solucions i característiques que serien difícils d'expressar amb la programació convencional. \n",
    "\n",
    "## Tipus diferents de Xarxes Neuronals\n",
    "### Topologia\n",
    "Existeixen moltes arquitectures diferents de xarxes neuronals segons la seva **topologia**. Alguns trets diferenciadors (no tots) poden ser: \n",
    "- Nombre de capes: hi ha xarxes que no tenen cap capa oculta (xarxa neuronal monocapa), mentre que d'altres en tenen moltes (xarxa neuronal multicapa); \n",
    "- Nombre de connexions entre neurones: hi ha xarxes que tenen totes les neurones connectades amb totes les neurones de la següent capa (xarxes \"*fully connected*\"), mentre que hi ha d'altres que només connecten algunes neurones; \n",
    "- Bucles: hi ha xarxes que permeten neurones auto-alimentades (la sortida és també l'entrada a la mateixa neurona) i que enlloc de capes permeten connexions arbitràries entre totes les neurones creant fins i tots bucles (RNN o xarxa neuronal recurrent), etc. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\" align=center>\n",
    "            <img src=\"Imatges/xarxes_neuronals_zoo.png\" width=500>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\" align=center>\n",
    "            Algunes topologies de xarxes neuronals. <br>\n",
    "            Font: Gràfic complet a <a href=\"https://www.asimovinstitute.org/neural-network-zoo/\">Neural Network Zoo</a>\n",
    "        </td>\n",
    "    </tr>   \n",
    "</table>\n",
    "\n",
    "\n",
    "### Tipus d'aprenentatge\n",
    "Però, a més a més de la topologia, també podem diferenciar les xarxes neuronals segons si el tipus d'aprenentatge emprat és supervisat o no supervisat.\n",
    "\n",
    "En l'aprenentatge **supervisat** es controla si la resposta generada és correcta o no i, si no ho és, modifica els pesos de les connexions per què el sistema sí que obtingui la sortida desitjada en la següent iteració (o s'apropi més a la sortida desitjada). Per calcular els nous valors dels pesos es pot fer servir:\n",
    "- aprenentatge per correcció d'error: s'utilitzen funcions d'error (també conegudes com funcions de pèrdua o *loss function* en anglès) per calcular la diferència entre el valor predit i el real i així poder decidir el nou pes.\n",
    "- o bé utilitzar el que es coneix com aprenentatge estocàstic, on es realitzen canvis aleatoris en els pesos i es comprova si els resultats són millors o pitjors (quedant-se el sistema només amb els canvis positius, òbviament)\n",
    "\n",
    "En l'aprenentatge **no-supervisat** o auto-supervisat el sistema no necessita d'una influència externa per ajustar els pesos. Ho aconsegueix buscant i trobant característiques, correlacions o categories que es puguin presentar entre les dades d'entrada. La sortida pot mostrar el grau de similitud entre les dades, les agrupacions (clustering) entre elles, o bé establir unes categories.\n",
    "\n",
    "Dins l'aprenentatge no supervisat ens podem trobar amb tècniques tan interessants com:\n",
    "- Aprenentatge Hebbià: mesura la familiaritat entre dades extraient les característiques de les mateixes.\n",
    "- Aprenentatge competitiu i comparatiu: es van afegint elements a una classe de manera incremental, si el nou element es similar a la classe actual es matisen els pesos de la xarxa, en cas contrari es crea una nova classe. Permet realitzar classificacions de les dades d'entrada.\n",
    "- Aprenentatge per reforç: enlloc de tenir un conjunt complet de dades correctes de sortida, només s'indica al sistema si la sortida és acceptable o no, i amb això l'algoritme ajusta els pesos fent servir un mecanisme de probabilitats.\n",
    "\n",
    "\n",
    "\n",
    "## Procés d'aprenentatge d'una xarxa neuronal\n",
    "\n",
    "Independentment de la topologia i el sistema d'aprenentatge escollit, tota implementació d'una xarxa neuronal necessita passar per aquests passos:\n",
    "\n",
    "1. Preparació de les dades: normalització i qualsevol pre-procés necessari de les dades d'entrada, així com la creació d'un subconjunt de dades d'entrenament i un altre de test o de validació.\n",
    "2. Escollir i configurar el model de xarxa neuronal: això inclou escollir la topologia, el nivell o nombre de capes ocultes, definir les funcions d'activació, les funcions de pèrdua, les iteracions màximes, etc.\n",
    "3. Entrenar: entrenar el sistema amb les dades d'entrenament.\n",
    "4. Avaluar: utilitzar el subconjunt de validació amb el sistema entrenat per avaluar els resultats. Si els resultats no són prou bons haurem de repetir l'entrenament canviant alguna configuració de la xarxa (augmentar o disminuir el nombre d'iteracions, canviar la funció d'activació, etc.)\n",
    "5. Producció: fer servir el model ja entrenat per a qualsevol dada d'entrada.\n",
    "\n",
    "\n",
    "Tots els passos són importants, però l'entrenament és pot ser el més costós i complicat ja que en sí és on el sistema \"aprèn\". Durant l'entrenament es calculen els pesos òptims per a la classificació; els que estan associats a cada connexió entre neurones. Aquests pesos determinaran el funcionament final de la xarxa i pot ser una tasca llarga i costosa a nivell computacional. La bona notícia és que després d'entrenar el sistema, els pesos i l'arquitectura de la xarxa es poden desar, i d'aquesta manera podem fer servir directament la xarxa en el futur sense necessitat d'entrenar-la cada cop. Això es coneix com una **xarxa pre-entrenada**. I és molt comú fer servir xarxes pre-entrenades per altres i adaptar-les a les nostres necessitats concretes amb el que es coneix com *fine tuning*.\n",
    "\n",
    "\n",
    "## Evolució de les Xarxes Neuronals: una mica d'història\n",
    "Les xarxes neuronals han anat evolucionant molts des dels seus inicis i han aparegut moltes arquitectures, topologies i algoritmes diversos al llarg dels anys. Per fer-vos una idea de la quantitat de sistemes diferents que han aparegut podeu consultar el gràfic complet del [Neural Network Zoo](https://www.asimovinstitute.org/neural-network-zoo/), un recurs molt gràfic i entenedor. Aquí us fem un molt breu resum dels moments i xarxes que han influït més en la història i l'evolució de les xarxes neuronals cap al deep learning. \n",
    "\n",
    "#### 1958: perceptró\n",
    "La primera neurona artificial va ser coneguda com *perceptró*, la qual tenia diverses entrades binàries i una única sortida, també binària. El perceptró també tenia pesos associats a cada entrada (números reals) que indicaven la importància de les respectives entrades. La sortida de la neurona (0 o 1) es calculava  partir de la suma de la multiplicació dels pesos per les entrades. També aplicava un llindar per activar la sortida.\n",
    "\n",
    "Es van fer servir per decisions binàries senzilles com funcions lògiques OR i AND.\n",
    "\n",
    "#### 1965: Perceptró multicapa\n",
    "Ampliació del perceptró en forma de xarxa neuronal, tot i que encara es feien servir només entrades i sortides binàries. Aquesta arquitectura ja permetia funcions lògiques més complexes com el XOR, però encara s'havien de configurar tots els pesos manualment.\n",
    "\n",
    "#### 1980': Aprenentatge automàtic\n",
    "Per aconseguir que les xarxes neuronals aprenguessin soles es va haver d'introduir noves neurones artificials no binàries: Van aparèixer les *Neurones Sigmoidals* que eren similars als perceptrons però que permetien que les entrades fossi valors reals o el que fos, enlloc de només valors binaris. \n",
    "\n",
    "Apareixen conceptes de xarxes Feedforward on la informació va sempre endavant (mai de forma cíclica sobre el mateix node), les xarxes totalment connectades (cada neurona està connectada amb totes i cadascuna de les neurones de la següent capa) i, un dels conceptes més trencadors: el **backpropagation**.\n",
    "\n",
    "L'algoritme de backpropagation va fer possible entrenar les xarxes neuronals de múltiples capes de manera supervisada. L'algoritme fa un primer pas feedforward per obtenir un primer resultat, en calcula l'error obtingut a la sortida i, llavors, va propagant cap a les capes anteriors (enrere), el càlcul de la mitjana de l'error i el fa servir per ajustar els pesos de cada capa. Això ho pot aplicar en diverses iteracions fins aconseguir l'objectiu desitjat en la sortida.\n",
    "\n",
    "Durant la segona meitat dels 80 van aparèixer les xarxes neuronals recurrents (RNN de l'anglès *Recurent Neural Networs*), les quals permeten fer connexions cap a capes anteriors, o fins i tot cap al mateix node creant cicles interns. Aquestes xarxes són útils quan es tracten dades històriques. \n",
    "\n",
    "#### 1989: Convolutional neural networks (CNN)\n",
    "Les CNN són xarxes multicapa amb diferents funcionalitats segons la capa. Les primeres capes extreuen característiques, les següents redueixen dimensions de les característiques extretes, les següents fan convolució, etc. Fins arribar a una sortida que activa la classificació resultant. Aquesta arquitectura fent servir capes profundes i la classificació de sortida van obrir un nou món en les xarxes neuronals. S'utilitzen molt en processament d'imatges, reconeixement de vídeo, i tasques de processament del llenguatge natural.\n",
    "\n",
    "Les veurem amb més detall en la següent secció.\n",
    "\n",
    "#### 1997: Long short term memory (LSTM)\n",
    "El LSTM és una RNN que permet nodes amb una memòria limitada. Les cel·les de memòria permeten a la xarxa recordar valors per períodes curts o llargs. I cada cel·la de memòria té comportes que controlen si la informació ha d'entrar, sortir o esborrar-se.\n",
    "\n",
    "Les LSTM han permès processar no tan sols dades unitàries (com imatges), sinó també seqüències de dades com el vídeo o la parla (reconeixement de veu text-to-speech, etc.)\n",
    "\n",
    "#### 2006: Deep Belief Networks (DBN): neix el Deep Learning\n",
    "A finals dels 90 principis dels 2000 l'ús de les xarxes neuronals es van estancar perquè es va arribar a uns nivells de profunditat de capes (centenars) que feia molt difícil i costós l'entrenament. Fins que al 2006 va arribar el DBN, el qual no va tenir moltes sortides però sí va permetre desencallar l'estat de les xarxes neuronals i començar a fer passes endavant cap a nous sistemes, cap al Deep Learning.\n",
    "\n",
    "Les DBN bàsicament van introduir el concepte de pre-entrenament de la xarxa amb algun mètode no supervisat, per obtenir uns valors d'inicialització dels pesos que evitessin l'estancament que obtenien quan s'entrenava amb una inicialització de pesos aleatòria. \n",
    "\n",
    "#### 2012: AlexNet\n",
    "El guanyador de la competició ILSVRC del 2012 va proposar una millora d’una xarxa CNN amb 8 capes ocultes i una combinació de mètodes. Va trigar una setmana a entrenar el sistema utilitzant GPUs però va ser una gran avenç quant a resultats de classificació i es van obrir les portes a la utilització pràctica del Deep Learning.\n",
    "\n",
    "#### 2014: Generative Adversarial Networks (GAN)\n",
    "Les GAN són xarxes que poden **crear** mostres noves quasi iguals a les mostres reals.\n",
    "\n",
    "Utilitza dues xarxes que entrena a la vegada i que competeixen entre elles: la xarxa Generadora i la Discriminadora. La generadora inicialment crea mostres a  partir de dades d'entrada \"brossa\" i les passa a la xarxa Discriminadora. Aquesta rep dades reals i dades de la Generadora i aprèn a diferenciar-les. L'objectiu final, després de moltes iteracions, és aconseguir que les dades generades no es puguin detectar o diferenciar de les dades reals.\n",
    "\n",
    "Un exemple real que utilitza xarxes GAN és la web [This Person Does Not Exist](https://www.thispersondoesnotexist.com/) que genera cares aleatòries cada vegada que refresques la pàgina. Les cares que mostra \"no existeixen\" però la majoria dels resultats són impressionants!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## CNN\n",
    "Com hem pogut veure el món de les xarxes neuronals artificials té uns quants anys d'història, i al llarg de tots aquests anys s'ha creat una gran diversitat de xarxes neuronals diferents. No podem estudiar cada xarxa en profunditat en aquest curs, però sí volem fer una aproximació a una de les més populars: la CNN.\n",
    "\n",
    "Les xarxes convolucionals o CNN (de l'anglès *Convolutional Neural Network*) són una de les xarxes neuronals més conegudes. És una arquitectura especial de xarxes neuronals proposada per Yann LeCun al 1998 i es basa en algunes propietats del còrtex visual dels animals.\n",
    "\n",
    "Inicialment es va crear pel reconeixement de lletres manuscrites, però a dia d’avui s’aplica a molts camps de reconeixement i classificació d’imatges, així com en reconeixement de vídeo i tasques de processament del llenguatge natural. Per exemple, Facebook fa servir CNN per etiquetar automàticament persones en les fotos, Amazon les fa servir per generar recomanacions de productes, Google per fer cerques en les fotos d’un usuari, etc. \n",
    "\n",
    "L’arquitectura consta de diverses capes de convolució que implementen l’extracció de característiques de la imatge i, posteriorment, unes capes completament connectades per aplicar la classificació. Totes les capes i tècniques aplicades aconsegueixen, a més de classificar correctament les imatges, tenir un algoritme invariant a canvis, escalats i distorsions de les imatges d'entrada.\n",
    "\n",
    "Arquitectura:\n",
    "1.\tInput\n",
    "2.\tConvolució + funció d’activació: extreu característiques.\n",
    "3.\tMax-pooling: redueix la dimensionalitat  de les característiques mantenint la informació més important.\n",
    "4.\tConvolució + funció d’activació + max-pooling de nou\n",
    "5.\tXarxa feedforward multicapa\n",
    "6.\tSortida: grup de nodes que classifiquen el resultat\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black; \">\n",
    "            <img src=\"Imatges/cnn_LeNet-5.png\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black;\" align=center>\n",
    "            Arquitectura LeNet-5: una CNN per a reconeixement de dígits. <br>\n",
    "            Font: Yann LeCun et al., <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-99.pdf\">\"Object recognition with Gradient-Based Learning\"</a>, 1999\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> \n",
    "    \n",
    "    \n",
    "\n",
    "### Convolució\n",
    "La fase de convolució aplica un filtre a un subconjunt de píxels dins la imatge Aquests filtres són matrius kernel normalment de dimensions 3x3 o 5x5.\n",
    " \n",
    "Per aplicar la convolució multipliquem la submatriu 3x3 de la imatge d’entrada per la matriu filtre 3x3 (obtenim un únic valor). Anirem movent i aplicant el filtre al llarg de tota la imatge. Realitzant el càlcul de la convolució en cada nova posició del filtre. \n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black; width:350px\">\n",
    "            <img src=\"Imatges/convolucio1.png\">\n",
    "        </td>\n",
    "        <td style=\"border:1px solid black; width:350px\">\n",
    "            <img src=\"Imatges/convolucio2.gif\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black;\" align=center>\n",
    " \tFont: <a href=\"https://www.guru99.com/convnet-tensorflow-image-classification.html\n",
    "\">Guru99</a>\n",
    "        </td>\n",
    "        <td style=\"border:1px solid black;\" align=center>\n",
    " \tCàlcul de la convolució en directe. <br>Font: <a href=\"https://www.guru99.com/convnet-tensorflow-image-classification.html\n",
    "\">Guru99</a>\n",
    "        </td>\n",
    "    </tr>   \n",
    "</table>\n",
    "\n",
    "\n",
    "La convolució permet emfatitzar els píxels més rellevants i eliminar el soroll irrellevant. El resultat serà una matriu de menys dimensions.\n",
    "\n",
    "Normalment les xarxes CNN tenen aquests paràmetres de configuració de la convolució:\n",
    "1.\tProfunditat (depth): configura el nombre de filtres a aplicar durant la convolució\n",
    "2.\tStride : defineix el nombre de píxels a saltar quan movem el filtre de convolució per la imatge (si és 1 anem lliscant el filtre píxel a píxel, si és 2 saltem 2 píxels, etc.). Quant més alt sigui aquest paràmetre, menys dimensions  tindran els mapes de característiques.\n",
    "3.\tZero-padding:  si apliquem el filtre centrat en cadascun dels píxels de la imatge o bé només on càpiga el filtre dins la imatge. En el primer cas afegirem files i columnes de zeros per poder processar els píxels de les cantonades i vores; si s’aplica així, el resultat tindrà les mateixes dimensions que la imatge d’entrada\n",
    "4.\tNon Linearity (**ReLU**): Funció d’activació que s’aplica al final de la convolució per permetre la no linealitat de les dades. La funció més utilitzada és la coneguda com ReLU que bàsicament canvia tots els píxels amb números negatius per zero.\n",
    "\n",
    "\n",
    "### Funció d’activació: ReLU\n",
    "\n",
    "Com ja hem comentat la funció d’activació s’aplica al final de la convolució per permetre la no linealitat de les dades. \n",
    "\n",
    "La funció més utilitzada és la coneguda com ReLU que bàsicament canvia tots els píxels amb números negatius per zero. Però n’existeixen d’altres tipus, com per exemple la Leaky ReLU que penalitza els píxels negatius mitjançant un coeficient rectificador (deixa els positiu igual i multiplica els negatius per un coeficient rectificador)\n",
    "\n",
    "\n",
    "### Max-pooling\n",
    "L’operació de max-pooling té l’objectiu de reduir la dimensionalitat de la imatge d’entrada i, així, poder reduir tant la complexitat de les operacions com la possibilitat d’overfitting (a menys dimensions, menys pesos a processar).\n",
    "\n",
    "Aquí de nou processem tota la imatge en submatrius (en aquest cas poden ser 2x2) i escollim el valor màxim o la mitjana del valor com a representant d’aquella submatriu de píxels.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black; width:400px\">\n",
    "            <img src=\"Imatges/max-pooling.png\">\n",
    "        </td>\n",
    "    </tr>   \n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\" align=center>\n",
    " \tmax-pooling. <br>Font: <a href=\"https://www.guru99.com/convnet-tensorflow-image-classification.html\n",
    "\">Guru99</a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "El max-pooling redueix les dimensions d’amplada i alçada de la matriu, però manté la profunditat de les dades.\n",
    "\n",
    "S'aplica tant a les fases d'entrenament com les de validació.\n",
    "\n",
    "### Dropout\n",
    "El dropout (descartar) és una tècnica de regularització que s'aplica només durant la fase d'entrenament; durant la fase d'avaluació el dropout no està actiu.\n",
    "\n",
    "L'objectiu principal del dropout és reduir i eliminar les dependències innecessàries i recurrents de la xarxa, descartant o ignorant aquelles neurones que no són essencials per l'objectiu de la xarxa. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black; width:400px\">\n",
    "            <img src=\"Imatges/dropout.png\">\n",
    "        </td>\n",
    "    </tr>   \n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\" align=center>\n",
    "            Dropout<br>\n",
    "            Font: Srivastava, Nitish, et al. <a href=\"http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer\n",
    "\">”Dropout: a simple way to prevent neural networks from\n",
    "overfitting”</a>, JMLR 2014\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Aplicant el dropout aconseguim que el model sigui més simple i més generalitzat, la qual cosa evita càlculs innecessaris i, també, redueix l'overfitting. \n",
    "\n",
    "Utilitzant un exemple metafòric, si estem entrenant una xarxa per aprendre a identificar imatges de gossos i la xarxa aprèn que si té una cua marró, 4 potes i orelles és un gos, el dropout aprendria després d'entrenar amb diverses imatges, que el color de la cua no importa i descartaria la neurona de la xarxa que busca aquesta característica.\n",
    "\n",
    "No s'ha de confondre el dropout amb el max-pooling (tot i que molta gent ho fa). El max-pooling no és una tècnica de regularització sinó que és una part de l'arquitectura de la xarxa i, per tant, s'utilitza tant durant la fase d'entrenament com en la fase de validació. L'objectiu del max-pooling és reduir la dimensionalitat de les dades, aconseguint que la xarxa sigui més invariant al soroll en les dades d'entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referències\n",
    "- Breu història de les xarxes neuronals: [https://www.aprendemachinelearning.com/breve-historia-de-las-redes-neuronales-artificiales/](https://www.aprendemachinelearning.com/breve-historia-de-las-redes-neuronales-artificiales/)\n",
    "\n",
    "- *Crea una Red Neuronal desde cero con Python*: [https://www.aprendemachinelearning.com/crear-una-red-neuronal-en-python-desde-cero/](https://www.aprendemachinelearning.com/crear-una-red-neuronal-en-python-desde-cero/), i el seu codi en acció aplicat a un cotxe arduino: [https://www.youtube.com/watch?v=_b2EdqjKTmU](https://www.youtube.com/watch?v=_b2EdqjKTmU)\n",
    " \n",
    "- *Image Classification using Convolutional Neural Networks in Keras*, de Vikas Gupta: [https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras/]( https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras/)\n",
    "   \n",
    "- *A Beginner's Guide To Understanding Convolutional Neural Networks*, de Adit Deshpande: [https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/)\n",
    "\n",
    "- Neural Network Zoo: breu explicació gràfica de les arquitectures de xarxes neuronals més comunes: [https://www.asimovinstitute.org/neural-network-zoo/](https://www.asimovinstitute.org/neural-network-zoo/)\n",
    "\n",
    "- Arquitectures CNN famoses: [https://towardsdatascience.com/the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96](https://towardsdatascience.com/the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96)\n",
    "\n",
    "-  Article CNN de Yann LeCun: *Object Recognition with Gradient-Based Learning*, Yann LeCun et al: [http://yann.lecun.com/exdb/publis/pdf/lecun-99.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-99.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
