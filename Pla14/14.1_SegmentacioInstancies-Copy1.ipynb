{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14.1 Segmentació d'instàncies\n",
    "\n",
    "Com ja havíem introduït en capítols anteriors, la segmentació d'imatges crea una imatge píxel a píxel dels objectes detectats i té dos nivells d'actuació: la segmentació semàntica i la segmentació d'instàncies. En la segmentació semàntica s'analitza la imatge a nivell de píxels, mentre que en la segmentació d'instàncies primer es fa una detecció d'objectes i posteriorment s'etiqueten els píxels dins l'objecte detectat.\n",
    "\n",
    "<table width=75%>\n",
    "    <tr>\n",
    "        <td align=center style=\"border:1px solid black\">\n",
    "            <img src=\"Imatges/cats.jpeg\"><br>\n",
    "            Original\n",
    "        </td>\n",
    "        <td align=center style=\"border:1px solid black\">\n",
    "            <img src=\"Imatges/cats_objdetection.png\"><br>\n",
    "            Detecció d'objectes\n",
    "        </td>\n",
    "    </tr>\n",
    "     <tr >\n",
    "        <td align=center style=\"border:1px solid black; background:white\">\n",
    "            <img src=\"Imatges/cats_instance_segmentation.jpeg\"><br>\n",
    "            Segmentació\n",
    "        </td>\n",
    "        <td align=center style=\"border:1px solid black; background:white\">\n",
    "            <img src=\"Imatges/cats_img_segmentation.png\"><br>\n",
    "            Segmentació d'instàncies\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=2 align=center style=\"border:1px solid black\">\n",
    "            <img src=\"Imatges/cats_panoptic.png\"><br>\n",
    "            Segmentació Panòptica\n",
    "        </td>\n",
    "    </tr>\n",
    "   <tr>\n",
    "        <td colspan=2 align=center style=\"border:1px solid black\">\n",
    "            <i>Font: <a href=\"https://medium.com/@danielmechea/what-is-panoptic-segmentation-and-why-you-should-care-7f6c953d2a6a\">What is Panoptic Segmentation and why you should care</a>, <br>de Daniel Mechea en Medium</i>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "       \n",
    "Abans de la segmentació d'instàncies els sistemes només detectaven objectes en imatges i els marcaven amb una capsa de mides i forma fixes, però això no era suficient per prendre decisions en molts sistemes com els de conducció automàtica o en les imatges mèdiques (entre d'altres), necessitaven saber-ne la forma exacta. L'aparició de la segmentació d'instàncies va tenir una gran repercussió en aquesta camps: saber la forma precisa d'un cotxe detectat podia ajudar al sistema de conducció automàtica a esquivar-lo amb precisió; saber la forma precisa de les cèl·lules podia ajudar a detectar les cèl·lules cancerígens de manera més eficaç, etc.\n",
    "\n",
    "\n",
    "Són molts els camps que treuen profit de la segmentació d'instàncies, entre ells podem anomenar:\n",
    "- Sistemes de control de trànsit\n",
    "- Conducció automàtica\n",
    "- Detecció d'objectes per satèl·lit\n",
    "- Biomedicina i imatges mèdiques \n",
    "\n",
    "## Mètriques per Segmentació d'Imatges\n",
    "Tot algoritme necessita algun tipus de mètrica d'error per comprovar com de bé està assolint el seu objectiu. En el cas de la segmentació d'imatges hi ha diverses mètriques que es poden aplicar, però sobretot podríem destacar-ne dues: \n",
    "\n",
    "### Pixel Accuracy\n",
    "El Pixel Accuracy mesura el percentatge de píxels que han estat segmentats correctament en una imatge.\n",
    "\n",
    "Tot i que sembla molt lògic d'entendre i aplicar, té un gran desavantatge quan les classes a detectar no estan equilibrades en una imatge. Per exemple, si mirem la imatge de sota, a l'esquerra tenim una imatge satèl·lit amb un port amb vaixells que volem detectar de manera automàtica amb segmentació d'imatges. A la dreta veiem el que s'anomena \"ground truth\", el que detectaríem manualment. \n",
    "\n",
    "<table width=75%>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"Imatges/vaixell-satellit.png\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Font: Article <a href=\"https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2\">Image Segmentation: Kaggle experience (Part 1 of 2) </a> de Vlad Shmyhlo en Towards Data Science \n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "El mar domina quasi tota la imatge, pel que més del 90% de la imatge pertany a la classe de fons, al mar.  Només que detectem correctament aquesta classe ja estarem tenint un resultat molt elevat del Pixel d'Accuracy, tant si detectem algun vaixell com si no.\n",
    "\n",
    "Aquest problema s'anomena \"desequilibri de classe\" (en anglès *class imbalance*) i és molt comú en moltes imatges del món real com en la conducció automàtica. Quan les classes a classificar estan molt desequilibrades, la mètrica Pixel Accuracy no ens pot donar valors fiables.\n",
    "\n",
    "\n",
    "### IoU o Jacquard Index\n",
    "\n",
    "La \"Intersecció sobre Unió\" (**IoU** de l'anglès *Intersection over Union*) també es coneix amb el nom de Jacquard Index i ja la vam introduir quan vam parlar de les mètriques d'avaluació per detecció d'objectes (mAP el capítol 10.3 Detecció d'objectes en una fase).\n",
    "\n",
    "<table width=75%>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\" width=45%><img src=\"Imatges/iou_equation.png\"></td>       \n",
    "        <td style=\"border:1px solid black\"><img src=\"Imatges/iou_examples.png\"></td>       \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border:1px solid black\" align=center>El càlcul de l'IoU és la divisió entre l'àrea d'intersecció i l'àrea d'unió de les capses base i predites.</td>\n",
    "        <td style=\"border:1px solid black\" align=center>Exemples de valors de IoU.</td>\n",
    "    </tr>  \n",
    "    <tr>\n",
    "        <td colspan=2 align=center style=\"border:1px solid black\">\n",
    "            Font: <a href=\"https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\">PyImageSearch</a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "La IoU és una mesura de precisió basada en la quantitat d'àrea de la de la finestra predicció que coincideix amb la finestra d'entrenament (o el \"ground truth\"). I és una de les mètriques més usades per a la segmentació d'imatges, calculant la mitjana del seu valor per a cada instància o classe a segmentar en la imatge. El seu valor està sempre entre 0 i 1 (essent 1 el màxim).\n",
    "\n",
    "\n",
    "\n",
    "# Algoritme U-NET\n",
    "\n",
    "Un dels algoritmes més coneguts per a la segmentació d'imatges és el que es coneix amb el nom de **U-Net** i va ser presentat per Olaf Ronneberger, Phillip Fischer, i Thomas Brox al 2015 com a proposta per a una millor segmentació d'imatges biomèdiques.\n",
    "\n",
    "Tot i que la U-Net va ser originalment creada per a la segmentació d'imatges mèdiques, funciona molt bé en una gran varietat de problemes de segmentació d'imatges naturals, des de segmentació de cèl·lules en imatges de microscopi fins a la detecció de vaixells i cases en fotos fetes per satèl·lit.\n",
    "\n",
    "La següent imatge mostra l'arquitectura de la U-Net:\n",
    "\n",
    "<table width=75%>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"Imatges/unet.png\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td align=center>\n",
    "            <i>Font: <a href=\"https://arxiv.org/pdf/1505.04597.pdf\">Olaf Ronneberger et al. \"U-Net: Convolutional Networks for Biomedical Image Segmentation\" (2015)</a></i>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "El nom de l'algoritme ve de la forma de \"U\" que pren la seva arquitectura. L'algoritme aplica capes, per ordre, des de la part superior esquerra, baixant fins la part inferior, per pujar després per la part dreta.\n",
    "\n",
    "La proposta consta de dues parts molt diferenciades en l'arquitectura: la codificació o compressió de dades (part esquerra que baixa) i la descodificació o descompressió de dades (part dreta que puja). \n",
    "\n",
    "La primera part, la codificació, s'utilitza per captar el context de la imatge i consta d'un conjunt de capes convolucionals i max pooling. La segona part, la descodificació, s'utilitza per localitzar amb precisió fent servir un conjunt de convolucions transposades. \n",
    "\n",
    "Arquitectura:\n",
    "- Codificació: Consta d'una arquitectura típica CNN amb capes consecutives de convolucions 3x3 (fletxa blava) seguida d'un max pooling 2x2 (fletxa vermella). A cada nova capa el nombre de canals es duplica.\n",
    "- Descodificació: Consta d'una convolució ascendent 2x2 (fletxa verda) i dues convolucions 3x3 (fletxa blava). A cada nova capa el nombre de canals es redueix a la meitat. Com es perden els píxels de les vores en cada convolució, en aquesta fase, després de cada capa, es produeix una concatenació de mapes de característiques amb la capa corresponent del camí de codificació (fletxes verdes), proporcionant aíxi informació de la localització.\n",
    "- Capa final: Una convolució 1x1 per associar el mapa de característiques amb el nombre desitjat de classes de sortida.\n",
    "\n",
    "Tot junt crea una FCN (Fully Convolutional Network), és a dir, una xarxa que només conté capes convolucionals, la qual cosa permet, entre d'altres avantatges, acceptar imatges de qualsevol tamany. La xarxa no té capes totalment connectades, només s'utilitzen les capes de convolució, cadascuna d'elles activada mitjançant una funció d'activació de ReLU.\n",
    "\n",
    "La funció de loss és una binary cross entropy loss ponderant més els píxels que pertanyen a les fronteres dels objectes. Podeu trobar més informació sobre aquesta funció loss en aquesta entrada [Understanding binary cross-entropy / log loss: a visual explanation](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) de TDS.\n",
    "\n",
    "Un dels avantatges de l'U-Net és que no necessita milers d'imatges d'entrenament per poder obtenir uns alts nivells de precisió. Com exemple, us mostrem en la següent imatge els resultats de l'aplicació d'una arquitectura U-Net per detectar glaucomes en imatges de retines; el glaucoma és una de les causes de la ceguera i una de les mesures per detectar-la és el rati entre el disc òptic i la copa òptica. La base de dades de l'exemple contenia només 101 imatges de retina, amb màscares d'anotació del disc òptic i la copa òptica. 50 imatges es van fer servir per entrenar el sistema i 51 per fer les inferències. El model va completar l'entrenament en 11 minuts 33 segons i va aconseguir un IoU de 0.8421.\n",
    "\n",
    "\n",
    "<table width=75%>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"Imatges/u-net-retines.jpeg\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Imatges no utilitzades durant l'entrenament per predir el disc òptic (vermell) i la copa òptica (groc)\n",
    "            <br>\n",
    "            Font: article <a href=\"https://towardsdatascience.com/biomedical-image-segmentation-u-net-a787741837fa\">Biomedical Image Segmentation: U-Net</a>, de Hong Jing (Jingles) en Towards Data Science.\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "# Mask R-CNN\n",
    "Finalment no podem tancar aquest capítol sense parlar d'un dels algoritmes més populars avui en dia per a la segmentació d'instàncies en imatges: el Mask R-CNN.\n",
    "\n",
    "En el capítol anterior vam explicar l'evolució rapidíssima que havien tingut els detectors d'objectes de dues fases en poc més de 3 anys: R-CNN (2014), Fast R-CNN (2015) i Faster R-CNN (2016). Doncs bé, l'evolució no es va parar aquí. Al 2017 va aparèixer el **Mask R-CNN** com a extensió del Faster R-CNN. \n",
    "\n",
    "El Faster R-CNN consta de dues etapes: una primera etapa anomenada RPN (Region Proposal Network) que proposa candidats a bounding boxes i, una segona etapa, on s'extreuen les característiques de cada candidat a bounding box (amb RoIPool) i on es fa una regressió per calcular la classificació (etiqueta de classe) i les coordenades del bounding box.\n",
    "\n",
    "El Mask R-CNN afegeix una tercera branca paral·lela en la segona etapa del Faster R-CNN per calcular la màscara de l'objecte detectat. D'aquesta manera s'aconsegueixen 3 sortides per a cada objecte detectat: l'etiqueta de classe, les coordenades de la bounding box i la màscara de l'objecte.\n",
    "\n",
    "<table width=75%>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"Imatges/mask-r-cnn.png\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"background:white\">\n",
    "            Resultats Mask R-CNN amb el conjunt de test COCO. Es mostren les màscares en color. També es mostren les bounding box i els valors de confiança.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td align=center>\n",
    "            <i>Font: <a href=\"http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf\">Kaiming He et al. Mask R-CNN (2017)</a></i>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Desavantatge principal: necessita molt temps d'entrenament.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referències\n",
    "\n",
    "- Mètriques d'error en Segmentació d'Imatges: [https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2](https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2)\n",
    "- Exemple aplicació U-Net per imatges biomèdiques: *Digital Pathology segmentation using Pytorch + U-NET*: [http://www.andrewjanowczyk.com/pytorch-unet-for-digital-pathology-segmentation/](http://www.andrewjanowczyk.com/pytorch-unet-for-digital-pathology-segmentation/)\n",
    "- Olaf Ronneberger, Philipp Fischer, and Thomas Brox. \"U-Net: Convolutional Networks for Biomedical Image Segmentation\" (2015): [https://arxiv.org/pdf/1505.04597.pdf](https://arxiv.org/pdf/1505.04597.pdf)\n",
    "- Kaiming He, Georgia Gkioxari, Piotr Dollar, Ross Girshick. \"*Mask R-CNN*\" (2017): [http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf)\n",
    "- Exemple d'aplicació Mask R-CNN amb Python: *Computer Vision Tutorial: Implementing Mask R-CNN for Image Segmentation (with Python Code)*, de Pulkit Sharma en Medium: [https://medium.com/analytics-vidhya/computer-vision-tutorial-implementing-mask-r-cnn-for-image-segmentation-with-python-code-fe34da5b99cd](https://medium.com/analytics-vidhya/computer-vision-tutorial-implementing-mask-r-cnn-for-image-segmentation-with-python-code-fe34da5b99cd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
