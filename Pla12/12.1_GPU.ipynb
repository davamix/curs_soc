{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.1 GPU\n",
    "\n",
    "## Què és una GPU?\n",
    "La **GPU** (de l'anglès *Graphics Processing Unit*) és una unitat de processament gràfic, un xip de microprocessament dissenyat originalment per gestionar tasques intensives de representació gràfica, la majoria de vegades per continguts en 3D, com en el cas dels jocs d'ordinador.\n",
    "\n",
    "#### I una CPU?\n",
    "Doncs funcionalment quasi el mateix que una GPU però amb dissenys bastant diferents. Tant la CPU com la GPU són processadors compostos per circuits integrats amb transistors dedicats a realitzar càlculs matemàtics a nivell binari. La diferència principal és que la CPU (de l'anglès *Central Processing Unit*) és una unitat de processament de *propòsit general*, és a dir que es fa servir per a fer qualsevol tipus de càlcul; mentre que la GPU és un processador de *propòsit específic*, és a dir que està optimitzat per treballar amb grans quantitats de dades però per realitzar les mateixes operacions més senzilles una vegada i una altra. En altres paraules, totes dues unitats fan càlculs, però la CPU està preparada tant per càlculs simples com complexos de manera \"seqüencial\", mentre que la GPU ho està per realitzar centenars o milers de càlculs simples de manera \"paral·lela\".\n",
    "\n",
    "La CPU està dissenyada amb pocs nuclis (o unitats de processament) però molt complexes i potents, i a altes freqüències de rellotge, que permeten executar alguns programes al mateix temps. La GPU, en canvi, té centeners o milers de nuclis senzills, a freqüències de rellotge relativament baixes, que permeten executar centenars o milers de programes específics a la vegada. És a dir, que una GPU pot tenir milers de nuclis però funcionant a velocitats inferiors que una CPU i processant informació més limitada que aquesta.\n",
    "\n",
    "<table width=50%>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"Imatges/cpu_vs_gpu.jpg\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Una GPU normalment s'encarrega de tasques amb un alt grau de paral·lelisme; aquelles que estan compostes per una instrucció i múltiples dades.\n",
    "\n",
    "Les principals unitats d'informació amb què tracta una GPU són vèrtexs i píxels i, de fet, els nuclis de la GPU es poden dividir principalment en dos tipus: els que processen vèrtexs i els que processen píxels.\n",
    "\n",
    "Tot i el tipus d'informació limitada que pot tractar una GPU, cal destacar que té una memòria molt més ràpida que una CPU, el que pot jugar un paper principal a l'hora d'emmagatzemar els resultats intermedis de les operacions i les textures que s'utilitzen.\n",
    "\n",
    "Una GPU amb els seus milers de nuclis treballant en paral·lel pot arribar a multiplicar per 100 el rendiment d'una CPU en operacions amb vectors i matrius, i amb vèrtexs i píxels.\n",
    "\n",
    "Per acabar-nos de fer una idea del que estem parlant, no us podeu perdre aquest vídeo on es compara metafòricament l'execució d'una CPU amb una GPU:\n",
    "\n",
    "<table align=center>\n",
    "    <tr>\n",
    "        <td align=center>\n",
    "            <a href=\"http://www.youtube.com/watch?v=-P28LKWTzrI\"><img src=\"http://img.youtube.com/vi/-P28LKWTzrI/0.jpg\"></a><br>Mona Lisa: CPU vs GPU\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### Les GPU són de recent creació?\n",
    "Tot i que darrerament molta gent parla de les GPU, no són un concepte nou, existeixen des de fa més de 30 anys! Les primeres targetes gràfiques van aparèixer al 1983 (la iSBX 275 d'Intel), i en el 1985 ja va aparèixer el primer ordinador amb la seva pròpia GPU (la Commodore Amiga). \n",
    "\n",
    "Un incís per remarcar que GPU i targeta gràfica \"no\" són termes intercanviables. La GPU, originalment, formava part de la targeta gràfica, era un component més d'aquesta, però ni era ni és per sí sola una targeta gràfica:\n",
    "- La paraula GPU fa referència al nucli de la targeta gràfica on es realitzen els càlculs matemàtics perquè el nostre ordinador pugui representar una imatge en la pantalla.\n",
    "- Mentre que la targeta gràfica fa referència al conjunt de tots els components de la targeta, incloent la GPU, els mòduls de memòria, els dissipadors, les sortides de vídeo, etc.\n",
    "\n",
    "Però continuem amb l'evolució històrica: durant els 90 les GPU van anar incorporant més capacitats i funcions, permetent les primeres renderitzacions de gràfics 2D al principi, i de 3D poc després. Les GPU van anar adquirint més protagonisme i permetent gràfics cada cop més realistes i espectaculars, tant en ordinadors personals com en consoles de joc com la PlayStation de la època. Des d'aleshores han aparegut gran millores i s'han creat APIs estàndards per facilitar la programació de les GPUs, com l'OpenGL i la Direct3D. \n",
    "\n",
    "Encara avui dia les GPU s'utilitzen principalment per produir gràfics digitals reals que permeten experiències de joc d'alta qualitat. Però gràcies a les GPU cada cop més potents, i a l'evolució en paral·lel de la intel·ligència artificial, les GPUs també s'han popularitzat molt en el món de la intel·ligència artificial per realitzar tasques que impliquin moltes computacions simples i simultànies, com és el cas de les xarxes neuronals de deep learning (entre d'alters), tant per la fase d'entrenament com per les fases d'inferència.\n",
    "\n",
    "Per tant, les GPUs no són noves, no són de recent creació, però sí que és nou el fet de fer-les servir per altres objectius diferents a la representació d'imatges.\n",
    "\n",
    "Hi ha diverses aplicacions comercials fora dels jocs que actualment es beneficien de les GPUs, sobretot en l'àmbit científic i de simulació:\n",
    "- El software de modelat 3D (per exemple AutoCAD) fa servir GPU per representar i renderitzar el model. Renderitzar un model és costós a nivell de càlculs i de temps, i cada cop que es fan múltiples canvis el model s'ha de renderitzar de nou. \n",
    "- L'edició de vídeo, sobretot quan es treballen amb gran quantitat d'arxius d'alta resolució, també es beneficia de les GPU per transcodificar els arxius a una velocitat raonable.\n",
    "- La implementació de simulació de fluids.\n",
    "- Els algoritmes de clusterització.\n",
    "- L'aprenentatge automàtic de xarxes neuronals és molt més ràpid amb GPUs enlloc de CPUs perquè poden processar més dades i càlculs en paral·lel.\n",
    "- Qualsevol aplicació que necessiti \"força bruta\" computacional també es beneficia de les GPUs: data mining, càlcul científic, anàlisis de dades de grans empreses, deep learning. \n",
    "\n",
    "## GPU vs CPU\n",
    "Actualment les GPUs s'utilitzen en multitud d'àmbits, des de l'acadèmic fins el financer. No sols perquè són més eficients en càlculs en paral·lel, sinó també pel seu baix cost en relació a la seva potència de càlcul i la seva optimització per a càlculs en coma flotant. Cada cop hi ha més àmbits on es prefereixen GPUs enlloc de CPUs.\n",
    "\n",
    "Hem de tenir en compte, però, que tot i que qualsevol algoritme implementable amb un CPU també es pot implementar amb una GPU, no sempre seran igual d'eficients. Només aquells algoritmes amb un alt grau de paral·lelisme, sense necessitat d'estructures de dades complexes i amb una alta intensitat aritmètica seran els que més es beneficiaran de fer servir GPUs. Però si s'ajunten totes aquestes condicions, el guany en temps de processament és enorme. \n",
    "\n",
    "Per exemple, a continuació podeu veure els temps de processament d'unes imatges de test amb una xarxa YOLOv3 fent servir CPUs i GPUs; la diferència de temps i la gran millora aconseguida amb GPU és molt remarcable:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>CPU</td>\n",
    "        <td>GPU</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src='./Imatges/cpu.png'></td>\n",
    "        <td><img src='./Imatges/gpu.png'></td>\n",
    "    </tr>\n",
    "    \n",
    "</table>\n",
    "\n",
    "\n",
    "## GPU i Deep Learning\n",
    "Les GPU aporten un gran avantatge computacional a les xarxes de deep learning gràcies al seu:\n",
    "- Elevat nombre de unitats de processament: Cada GPU té un gran nombre de nuclis, el què permet optimitzar el càlcul de múltiples processos paral·lels.\n",
    "- Elevat ample de banda de la memòria: El deep learning fa servir grans quantitats de dades en els seus càlculs, per la qual cosa una GPU s'adapta millor que una CPU perquè pot tenir un ample de banda de la memòria 10x més (una GPU por funcionar fins a 750GB/s davant els 50GB/s de la CPU tradicional).\n",
    "\n",
    "Però no totes les GPU són iguals, tenen característiques variables d'un model a un altre, el que pot fer que algunes siguin més adients que d'altres segons el tipus de xarxa de deep learning que vulguem entrenar. Estudiar bé el tipus de problema a processar ens pot ajudar a escollir la GPU més idònia pel nostre sistema. A continuació us llistem les característiques bàsiques a tenir en compte.\n",
    "\n",
    "**Característiques GPU:**\n",
    "- Ample de banda de la memòria: pot ser la característica més important. Escollir sempre la que tingui més ample de banda.\n",
    "- Nombre d'unitats de processament o nuclis (*cores*): El nombre de nuclis determina la velocitat amb què la GPU pot processar les dades, a més nuclis més velocitat.\n",
    "- Mida de la VRAM (RAM de Vídeo): és una mida de la quantitat de dades que la GPU pot gestionar a la vegada. Depenent de la tasca a calcular ens pot interessar una VRAM alta, però no sempre és important.\n",
    "- Potència de processament: indica la velocitat amb què la GPU pot calcular dades i determina amb quina velocitat realitzarà les tasques del sistema. Es calcula com un factor del nombre de nuclis multiplicat per la velocitat del rellotge a la qual s'executen. Una freqüència de rellotge estàndard avui en dia en les GPU és de 1-1,5GHz, mentre que per una CPU és del voltant de 3,8-4GHz en els models més potents. La GPU té una freqüència de rellotge molt més baixa, però gràcies a la seva arquitectura en paral·lel (al seu nombre elevat de nuclis) fa que acabi tenint una potència de càlcul molt més gran que la CPU.\n",
    "- GPU vs Cloud-GPU (multi-GPU): La GPU va ser creada per fer molts càlculs ràpids amb dades relativament petites, però les cloud GPU es van crear per portar a terme càlculs més llargs i pesats amb dades molt més grans. Entrenar un xarxa deep learning amb gigabytes de dades serà, de llarg, molt més ràpid en un cloud GPU que no pas amb una única GPU; però utilitzar el cloud GPU també per fer inferències pot ser no tindrà tant bons resultats perquè els càlculs seran més senzills i curts, i el volum de dades molt menor.\n",
    "\n",
    "Podeu veure una comparativa de temps de processament amb diferents tipus de GPU en aquesta entrada: [Comparing Object detection models’ performance on different GPUs](https://medium.com/datadriveninvestor/comparing-object-detection-models-gpus-935754ae2677).\n",
    "\n",
    "\n",
    "# VPU\n",
    "Tot i que la combinació GPUs aplicades al Deep Learning sembla que acabi d'arribar, la roda de l'evolució tecnològica no para mai. Amb les deep learning processant imatges en temps real, amb les xarxes neuronals cada cop més eficients en el reconeixement d'escenes, el següent pas semblava lògic: portar tota aquesta potència de càlcul i aquestes funcionalitats de gamma alta en temps real a tot dispositiu tecnològic disponible que tingui connectada una càmara: ordinadors, tauletes, mòbils, cotxes, robots... Per exemple, poder gravar des del mòbil i detectar i reconèixer en temps real objectes, o interpretar escenes pot ser de gran utilitat tant pels usuaris finals com per diferents companyies de serveis. Els algoritmes ja els teníem, ens faltava el maquinari potent i de baix cost que permetés la migració a dispositius connectats, que portés la intel·ligència a les càmeres: i no fa gaire van arribar les **Vision** Processsing Unit (**VPU**), un nou xip específic per optimitzar i accelerar l'entrenament i les inferències de les xarxes neuronals de deep learning.\n",
    "\n",
    "Compte! No confondre les \"Vision\" Processing Unit (VPU) amb les \"Vídeo\" Processing Unit, que no és més que un altre nom per les GPU (no sempre l'encerten a l'hora de buscar noms en el món tecnològic...).\n",
    "\n",
    "Les VPU fa poc més de 2 anys que estan entre nosaltres i estan revolucionant el sector.\n",
    "\n",
    "Xips dedicats de propòsit específic que poden gestionar aplicacions de visió per computador en temps real amb la potència d'un dispositiu mòbil! \n",
    "\n",
    "El primer processador de visió és el [Intel® Movidius™ Myriad™ 2 VPU](https://www.movidius.com/solutions/vision-processing-unit), un xip compost per :\n",
    "- interfícies de la càmera\n",
    "- conjunt de hardware de visió i sistema de processament de senyal d'imatge\n",
    "- 12 nuclis o processadors shave (vector vilw)\n",
    "- Una unitat central de memòria que facilita la connexió de la resta de sistemes i permet l'alta potència de càlcul a baix cost.\n",
    "\n",
    "<table align=center width=75%>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"Imatges/vpu_myriadX.png\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "    \n",
    "El xip permet ser programat i personalitzat per obtenir els objectiu desitjats. Inclou una llibreria de xarxes neuronals però també permet afegir les personals de l'usuari i combinar-les amb les de la llibreria en l'ordre que es vulgui.\n",
    "\n",
    "No sols pot captar imatges, també les processa i \"entén\" què hi ha en l'escena: des de la captació d'imatges, el processament dels píxels, passant pel processament de xarxes neuronals, fins al processament avançat de visió per interpretar les escenes amb diverses localitzacions i mapejos simultanis en temps real.\n",
    "\n",
    "Definitivament les VPU poden canviar el món dels dispositius connectats a una càmera, ja siguin mòbils, drons, robots o molt més.\n",
    "\n",
    "Estarem atents!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVINO\n",
    "Finalment, us volem introduir OpenVINO, una eina de Intel per integrar processament de vídeo i intel·ligència artificial. \n",
    "\n",
    "OpenVINO ve del seu nom en anglès *Visual Inferencing and Neural Network Optimization* i s'utilitza exclusivament per la fase d'inferències de xarxes neuronals de Deep Learning, ja sigui per detecció d'objectes, segmentació, classificació o qualsevol altra tasca de visió per computador. No s'utilitza per l'entrenament, parteix sempre d'una xarxa ja entrenada (tot i que es poden fer servir algunes dels models pre-entrenats que inclou la mateixa eina).\n",
    "\n",
    "OpenVINO està disponible per a múltiples plataformes, i es pot fer servir per utilitzar Intel CPU, GPUs integrades, VPU de Movidius o Intel FPGAs, o fins i tot una combinació de totes elles.\n",
    "\n",
    "Si voleu saber més de la versió Intel de l'OpenVINO podeu mirar [aquest vídeo](https://www.youtube.com/watch?v=kY9nZbX1DWM) o accedir a la [documentació aquí](https://docs.openvinotoolkit.org/).\n",
    "\n",
    "<img src='./Imatges/openvino.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referències\n",
    "\n",
    "Exemple pràctic de GPU per xarxes deep learning: [Training Deep Neural Networks on a GPU with PyTorch](https://medium.com/dsnet/training-deep-neural-networks-on-a-gpu-with-pytorch-11079d89805)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
